name: Hourly Orderflow Upload

on:
  schedule:
    # Runs every hour at minute 0 UTC (adjust timezone if needed)
    - cron: '0 * * * *' 
  workflow_dispatch: # Manual trigger support

jobs:
  upload-orderflow:
    runs-on: ubuntu-latest

    env:
      RENDER_API_BASE: https://comoflo.onrender.com/api  # Flask API base URL
      GITHUB_REPO: vishtheendodoc/comoflo               # Your GitHub username/repo
      DATA_FOLDER: data_snapshots
      GITHUB_TOKEN: ${{ secrets.MY_PAT }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          pip install requests pandas PyGithub

      - name: Fetch and upload data
        run: |
          python - <<EOF
          import requests
          import pandas as pd
          from datetime import datetime
          from github import Github
          import os
          import time

          # --- Config ---
          api_base = os.getenv("RENDER_API_BASE")
          api_stocks = f"{api_base}/stocks"
          api_data = f"{api_base}/delta_data/"
          repo_name = os.getenv("GITHUB_REPO")
          token = os.getenv("GITHUB_TOKEN")
          now = datetime.utcnow()
          filename = f"orderflow_{now.strftime('%Y%m%d_%H')}.csv"
          remote_path = f"{os.getenv('DATA_FOLDER')}/{filename}"

          # --- Helper: Retry GET requests ---
          session = requests.Session()
          adapter = requests.adapters.HTTPAdapter(max_retries=5)
          session.mount("https://", adapter)
          session.mount("http://", adapter)

          def fetch_json(url, timeout=60):
              for attempt in range(1, 6):
                  try:
                      print(f"ðŸ”„ Attempt {attempt}: {url}")
                      resp = session.get(url, timeout=timeout)
                      resp.raise_for_status()
                      return resp.json()
                  except Exception as e:
                      print(f"âš ï¸ Attempt {attempt} failed: {e}")
                      if attempt < 5:
                          time.sleep(5 * attempt)  # exponential backoff
              print(f"âŒ Failed after 5 attempts: {url}")
              return None

          # --- Fetch security IDs ---
          print("ðŸ“¡ Fetching stock list...")
          stock_list = fetch_json(api_stocks)
          if not stock_list:
              print("âŒ Could not fetch stock list. Exiting.")
              exit(1)

          security_ids = [str(s['security_id']) for s in stock_list if 'security_id' in s]
          print(f"âœ… Found {len(security_ids)} securities")

          # --- Fetch data for each security (with OI columns) ---
          combined_df = pd.DataFrame()
          skipped = []
          
          # Track OI data availability
          oi_columns_found = False
          
          for sid in security_ids:
              data = fetch_json(f"{api_data}{sid}?interval=1")
              if data:
                  df = pd.DataFrame(data)
                  df['security_id'] = sid
                  
                  # Check if OI columns exist in the data
                  if not oi_columns_found and any(col in df.columns for col in ['open_interest', 'oi_change', 'oi_interpretation']):
                      oi_columns_found = True
                      print("âœ… OI columns detected in data")
                  
                  combined_df = pd.concat([combined_df, df], ignore_index=True)
              else:
                  print(f"âš ï¸ Skipped {sid} due to errors")
                  skipped.append(sid)

          if combined_df.empty:
              print("âš ï¸ No data fetched for any security. Skipping upload.")
              exit(0)

          # --- Ensure consistent column order (with OI columns) ---
          # Base columns (always present)
          base_columns = [
              'security_id',
              'timestamp',
              'buy_volume',
              'sell_volume',
              'buy_initiated',
              'sell_initiated',
              'tick_delta',
              'open',
              'high',
              'low',
              'close',
              'delta',
              'inference'
          ]
          
          # OI columns (if present)
          oi_columns = ['open_interest', 'oi_change', 'oi_interpretation']
          
          # Build final column list based on what's available
          final_columns = []
          for col in base_columns:
              if col in combined_df.columns:
                  final_columns.append(col)
          
          # Add OI columns if they exist
          for col in oi_columns:
              if col in combined_df.columns:
                  final_columns.append(col)
          
          # Reorder dataframe
          combined_df = combined_df[final_columns]
          
          # Log column summary
          print(f"ðŸ“Š Columns in backup: {', '.join(final_columns)}")
          if oi_columns_found:
              print("âœ… Backup includes OI data")
          else:
              print("â„¹ï¸ Backup does not include OI data (legacy format)")

          # --- Save CSV atomically ---
          temp_filename = f"{filename}.tmp"
          combined_df.to_csv(temp_filename, index=False)
          os.replace(temp_filename, filename)
          print(f"âœ… Saved snapshot atomically: {filename} ({len(combined_df)} rows)")

          # --- Upload with retry ---
          g = Github(token)
          repo = g.get_repo(repo_name)

          def upload_file_with_retry(repo, remote_path, local_filename, commit_msg, max_retries=3):
              content = open(local_filename, "rb").read()
              for attempt in range(1, max_retries + 1):
                  try:
                      try:
                          contents = repo.get_contents(remote_path)
                          repo.update_file(contents.path, commit_msg, content, contents.sha)
                          print(f"âœ… Updated {remote_path}")
                      except Exception as e:
                          # Try create file if not exists (404)
                          if hasattr(e, 'status') and e.status == 404:
                              repo.create_file(remote_path, commit_msg, content)
                              print(f"âœ… Created {remote_path}")
                          else:
                              raise
                      return True
                  except Exception as e:
                      print(f"âš ï¸ Upload attempt {attempt} failed: {e}")
                      if attempt < max_retries:
                          wait_seconds = 10 * attempt
                          print(f"â³ Retrying upload in {wait_seconds} seconds...")
                          time.sleep(wait_seconds)
                      else:
                          print(f"âŒ Giving up after {max_retries} attempts.")
                          return False

          # Create commit message with metadata
          commit_msg = f"Backup {filename}"
          if oi_columns_found:
              commit_msg += " [with OI data]"
          
          success = upload_file_with_retry(repo, remote_path, filename, commit_msg)
          if not success:
              print(f"âŒ Backup upload failed for {filename}")
              exit(1)

          print(f"ðŸŽ‰ Backup completed successfully: {filename}")
          print(f"ðŸ“ˆ Total rows: {len(combined_df)}")
          print(f"ðŸ“Š Total securities: {len(security_ids) - len(skipped)}")
          if skipped:
              print(f"âš ï¸ Skipped securities ({len(skipped)}): {skipped[:10]}{'...' if len(skipped) > 10 else ''}")
          EOF
